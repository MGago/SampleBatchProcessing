{"name":"Samplebatchprocessing","tagline":"Sample Implementation of Batch Processing on Amazon Web Services (AWS)","body":"# Sample Implementation of Batch Processing on Amazon Web Services (AWS)\r\n\r\nThis is a Sample Implementation for the [AWS Reference Architecture for Batch Processing](http://aws.amazon.com/architecture/).\r\n\r\nIs is implemented in Python, using [boto](http://aws.amazon.com/sdkforpython/), and the new [AWS Command Line Interface (CLI)](http://aws.amazon.com/cli/).\r\n\r\nTwo tools are provided:\r\n* SendJobs.py - to upload files from a (local) directory to S3 and put \"job\" requests to process those files as messages in an SQS queue\r\n* GetJobs.py - to get \"job\" messages from an SQS queue and upload on S3 the outcome of the processing\r\n\r\nThe setup leverages [EC2](http://aws.amazon.com/ec2/) [Auto Scaling](http://aws.amazon.com/autoscaling/) to have a group of instances that is empty (i.e. no instance is running) when there are no \"job\" requests in the SQS queue and grows when there is the need.\r\n\r\n## Tutorial\r\n\r\n### Install AWS CLI\r\n\r\nThe new [AWS Command Line Interface (CLI) tool](http://aws.amazon.com/cli/)\r\nis Python based, so you can install it using \"pip\"\r\n\r\n    pip install awscli\r\n\r\nor using \"easy_install\"\r\n\r\n    easy_install awscli\r\n\r\nBefore using AWS CLI, you first need to specify your AWS account credentials and default AWS region as described\r\n[here](http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html).\r\n\r\nThe awscli package includes a very useful command completion feature,\r\ne.g. to enable tab completion for bash use the built-in command complete (not boot persistant):\r\n\r\n    complete -C aws_completer aws\r\n\r\n### Create an S3 Bucket to host input and output files\r\n\r\nYou can create a bucket from the [S3 web console](http://console.aws.amazon.com/s3/) or using the CLI:\r\n\r\n    aws s3 create-bucket --bucket  <S3 Bucket Name> \\\r\n    --create-bucket-configuration '{ \"location_constraint\": <Your AWS Region, e.g. \"eu-west-1\"> }'\r\n\r\n### Create an SQS Queue to centralize \"job\" requests\r\n\r\nYou can create a queue from the [SQS web console](http://console.aws.amazon.com/sqs/) or using the CLI:\r\nThe \"VisibilityTimeout\" is expressed in seconds and should be larger than the maximun processing time required for a \"job\".\r\nIt can eventually be increased for a single \"job\", but that is not part of this implementation.\r\n\r\n    aws sqs create-queue --queue-name <SQS Queue Name> --attributes VisibilityTimeout=60\r\n\r\n### Create a IAM Role to delegate access to processing instances\r\n\r\nFrom the [IAM web console](http://console.aws.amazon.com/iam/) -> Roles -> Create Role -> \r\nWrite a role name.Under \"AWS Service Roles\" select \"Amazon EC2\".\r\nSelect a \"Custom Policy\", write a policy name and see the \"role.json\" file\r\nfor a sample role giving access to an S3 bucket and an SQS queue.\r\nYou should replace \"AWS Account\", \"S3 Bucket Name\" and \"SQS Queue Name\" in the policy with yours.\r\nWrite down the Instance Profile ARN from the Summary tab, you'll need it later.\r\n\r\n### Create Auto Scaling Launch Configuration\r\n\r\nFor this sample I'm using a default Amazon Linux EBS-backed AMI, you can take the AMI ID [here](http://aws.amazon.com/amazon-linux-ami)\r\nThe user data script provided automatically configures and run multiple parallel \"GetJobs.py\" scripts per node to get \"job\" from the queue and process them, uploading the final result back on S3. You probably need to edit the \"user-data.sh\" file before launching the following command.\r\nAlternatively you can create your own AMI that starts one of more parallel \"GetJobs.py\" scripts at boot.\r\n\r\n    aws autoscaling create-launch-configuration --launch-configuration-name asl-batch \\\r\n    --image-id <Amazon Linux AMI ID> --instance-type <EC2 Instance Type, e.g. t1.micro> \\\r\n    --iam-instance-profile <Instance Profile ARN> --user-data \"`cat user-data.sh`\"\r\n\r\nIf you want to be able to login into the instances launched by Auto Scaling you can add the following parametrs to the previous command\r\n\r\n    --key-name <EC2 Key Pair for SSH login> --security-groups <EC2 Security Group allowing SSH access>\r\n\r\n### Create Auto Scaling Group\r\n\r\n    aws autoscaling create-auto-scaling-group --auto-scaling-group-name asg-batch \\\r\n    --launch-configuration-name asl-batch --min-size 0 \\\r\n    --max-size <Number of Instances to start when there are \"jobs\" in the SQS queue> \\\r\n    --availability-zones <All AZs in the region, \\\r\n    e.g. for \"eu-west-1\" you can use \"eu-west-1a\" \"eu-west-1b\" \"eu-west-1c\"> \\\r\n    --default-cooldown 300\r\n\r\n### Create Auto Scaling \"Up\" Policy\r\n\r\n    aws autoscaling put-scaling-policy --auto-scaling-group-name asg-batch --policy-name ash-batch-upscale-policy \\\r\n    --scaling-adjustment <Number of Instances to start when there are \"jobs\" in the SQS queue> \\\r\n    --adjustment-type ExactCapacity\r\n\r\nWrite down the \"PolicyARN\", you need it in the next step to set up the alarm.\r\n\r\n### Create CloudWatch Alarm to trigger \"Up\" scaling Policy\r\n\r\n    aws cloudwatch put-metric-alarm --alarm-name StartBatchProcessing --metric-name ApproximateNumberOfMessagesVisible \\\r\n    --namespace \"AWS/SQS\" --statistic Average --period 60  --evaluation-periods 2 --threshold 1 \\\r\n    --comparison-operator GreaterThanOrEqualToThreshold --dimensions name=QueueName,value=batch-queue \\\r\n    --alarm-actions <\"Up\" PolicyARN>\r\n\r\n### Create Auto Scaling \"Down\" Policy\r\n\r\n    aws autoscaling put-scaling-policy --auto-scaling-group-name asg-batch --policy-name ash-batch-downscale-policy \\\r\n    --scaling-adjustment 0 --adjustment-type ExactCapacity\r\n\r\nWrite down the \"PolicyARN\", you need it in the next step to set up the alarm.\r\n\r\n### Create CloudWatch Alarm to trigger \"Down\" scaling Policy\r\n\r\n    aws cloudwatch put-metric-alarm --alarm-name StopBatchProcessing --metric-name ApproximateNumberOfMessagesVisible \\\r\n    --namespace \"AWS/SQS\" --statistic Average --period 60  --evaluation-periods 2 --threshold 0 \\\r\n    --comparison-operator LessThanOrEqualToThreshold --dimensions name=QueueName,value=batch-queue \\\r\n    --alarm-actions <\"Down\" PolicyARN>\r\n\r\n### Send the jobs uploading files from a directory\r\n\r\nThe directory can be local or on an EC2 instance.\r\n\r\n    ./SendJobs.py <Directory> <S3 Bucket Name> input/ output/ <SQS Queue Name> <AWS Region, e.g. \"eu-west-1\">\r\n\r\nTo get help, run the tool without options\r\n\r\n    ./SendJobs.py\r\n\r\nAfter a few minutes the first CloudWatch Alarm should trigger the \"Up\" scaling Policy\r\nto start EC2 Instances configured to consume \"jobs\" from the SQS queue.\r\nWhen all \"jobs\" are processed and the SQS is \"empty\" the second CloudWatch Alarm should trigger\r\nthe \"Down\" scaling Policy to shutdown and terminate the EC2 Instances.\r\nYou should find the output of the processing in the S3 bucket under the \"ouput/\" prefix.\r\n\r\n### Change the Launch Configuration of an Auto Scaling Group\r\n\r\nIf later on you need to change the Launch Configuration create a new one and update the Auto Scaling Group, e.g.\r\n\r\n    aws autoscaling update-auto-scaling-group --launch-configuration-name asl-batch-v2 \\\r\n    --auto-scaling-group-name asg-batch\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}